{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The interpretations using LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:11.117172Z",
     "start_time": "2020-10-08T01:47:08.811105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age         float64\n",
      "sex         float64\n",
      "cp          float64\n",
      "trestbps    float64\n",
      "chol        float64\n",
      "fbs         float64\n",
      "restecg     float64\n",
      "thalach     float64\n",
      "exang       float64\n",
      "oldpeak     float64\n",
      "slope       float64\n",
      "ca          float64\n",
      "thal        float64\n",
      "target        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, date\n",
    "import datetime as dt\n",
    "import sys\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "\n",
    "heart = pd.read_csv('data/datasets_33180_43520_heart.csv', sep = ',')\n",
    "heart = explanation.to_float(heart)\n",
    "\n",
    "print(heart.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T16:46:35.990489Z",
     "start_time": "2020-10-07T16:46:35.978281Z"
    }
   },
   "source": [
    "# Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:11.146467Z",
     "start_time": "2020-10-08T01:47:11.120049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed acidity           float64\n",
      "volatile acidity        float64\n",
      "citric acid             float64\n",
      "residual sugar          float64\n",
      "chlorides               float64\n",
      "free sulfur dioxide     float64\n",
      "total sulfur dioxide    float64\n",
      "density                 float64\n",
      "pH                      float64\n",
      "sulphates               float64\n",
      "alcohol                 float64\n",
      "target                    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, date\n",
    "import datetime as dt\n",
    "import sys\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "\n",
    "\n",
    "wine = pd.read_csv('data/wine_limpo.csv', sep = ',')\n",
    "wine.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "\n",
    "wine = explanation.to_float(wine)\n",
    "\n",
    "print(wine.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:11.169747Z",
     "start_time": "2020-10-08T01:47:11.149511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies                 float64\n",
      "Glucose                     float64\n",
      "BloodPressure               float64\n",
      "SkinThickness               float64\n",
      "Insulin                     float64\n",
      "BMI                         float64\n",
      "DiabetesPedigreeFunction    float64\n",
      "Age                         float64\n",
      "target                        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, date\n",
    "import datetime as dt\n",
    "\n",
    "import sys\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "\n",
    "\n",
    "diabetes = pd.read_csv('data/diabetes_limpo.csv', sep = ',')\n",
    "diabetes.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "\n",
    "diabetes = explanation.to_float(diabetes)\n",
    "\n",
    "print(diabetes.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing 4 instances (TP, FP, TN, FN) of each dataset randomly from DT to be our baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to choose 4 instances (TP, FP, TN, FN) of each dataset randomly, we have choosed the decision tree as the baseline classificator to choose the confusion matrix metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the samples from the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:12.632365Z",
     "start_time": "2020-10-08T01:47:11.172257Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "heart = pd.read_csv('data/datasets_33180_43520_heart.csv', sep = ',')\n",
    "model = serializer.load_model(file_name = 'serialized_model/DT_heart.pkl')\n",
    "heart_sample = explanation.sample_baseline_model(model, heart)\n",
    "\n",
    "wine = pd.read_csv('data/wine_limpo.csv', sep = ',')\n",
    "model = serializer.load_model(file_name = 'serialized_model/DT_wine.pkl')\n",
    "wine_sample = explanation.sample_baseline_model(model, wine)\n",
    "\n",
    "diabetes = pd.read_csv('data/diabetes_limpo.csv', sep = ',')\n",
    "model = serializer.load_model(file_name = 'serialized_model/DT_diabetes.pkl')\n",
    "diabetes_sample = explanation.sample_baseline_model(model, diabetes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart dataset - explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:14.093981Z",
     "start_time": "2020-10-08T01:47:12.634326Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "heart = pd.read_csv('data/datasets_33180_43520_heart.csv', sep = ',')\n",
    "heart = explanation.to_float(heart)\n",
    "\n",
    "model_file_name = 'serialized_model/MLP_heart.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = heart.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in heart_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine dataset - explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:15.583400Z",
     "start_time": "2020-10-08T01:47:14.097724Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = pd.read_csv('data/wine_limpo.csv', sep = ',')\n",
    "# wine.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "wine = explanation.to_float(wine)\n",
    "\n",
    "model_file_name = 'serialized_model/MLP_wine.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = wine.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in wine_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:16.780280Z",
     "start_time": "2020-10-08T01:47:15.585536Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diabetes = pd.read_csv('data/diabetes_limpo.csv', sep = ',')\n",
    "# diabetes.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "diabetes = explanation.to_float(diabetes)\n",
    "\n",
    "model_file_name = 'serialized_model/MLP_diabetes.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = diabetes.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in diabetes_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:19.212924Z",
     "start_time": "2020-10-08T01:47:16.783747Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "heart = pd.read_csv('data/datasets_33180_43520_heart.csv', sep = ',')\n",
    "heart = explanation.to_float(heart)\n",
    "\n",
    "model_file_name = 'serialized_model/RF_heart.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = heart.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in heart_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:21.492281Z",
     "start_time": "2020-10-08T01:47:19.215064Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = pd.read_csv('data/wine_limpo.csv', sep = ',')\n",
    "# wine.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "wine = explanation.to_float(wine)\n",
    "\n",
    "model_file_name = 'serialized_model/RF_wine.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = wine.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in wine_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:23.152564Z",
     "start_time": "2020-10-08T01:47:21.494046Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diabetes = pd.read_csv('data/diabetes_limpo.csv', sep = ',')\n",
    "# diabetes.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "diabetes = explanation.to_float(diabetes)\n",
    "\n",
    "model_file_name = 'serialized_model/RF_diabetes.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = diabetes.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in diabetes_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:24.644630Z",
     "start_time": "2020-10-08T01:47:23.154207Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "heart = pd.read_csv('data/datasets_33180_43520_heart.csv', sep = ',')\n",
    "heart = explanation.to_float(heart)\n",
    "\n",
    "model_file_name = 'serialized_model/NaiveBayes_heart.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = heart.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in heart_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:26.169922Z",
     "start_time": "2020-10-08T01:47:24.647054Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = pd.read_csv('data/wine_limpo.csv', sep = ',')\n",
    "# wine.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "wine = explanation.to_float(wine)\n",
    "\n",
    "model_file_name = 'serialized_model/NaiveBayes_wine.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = wine.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in wine_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:27.349998Z",
     "start_time": "2020-10-08T01:47:26.172815Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diabetes = pd.read_csv('data/diabetes_limpo.csv', sep = ',')\n",
    "# diabetes.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "diabetes = explanation.to_float(diabetes)\n",
    "\n",
    "model_file_name = 'serialized_model/NaiveBayes_diabetes.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = diabetes.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in diabetes_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:29.204141Z",
     "start_time": "2020-10-08T01:47:27.352025Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "heart = pd.read_csv('data/datasets_33180_43520_heart.csv', sep = ',')\n",
    "heart = explanation.to_float(heart)\n",
    "\n",
    "model_file_name = 'serialized_model/DT_heart.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = heart.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in heart_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:31.170548Z",
     "start_time": "2020-10-08T01:47:29.206230Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = pd.read_csv('data/wine_limpo.csv', sep = ',')\n",
    "# wine.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "wine = explanation.to_float(wine)\n",
    "\n",
    "model_file_name = 'serialized_model/DT_wine.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = wine.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in wine_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:32.670401Z",
     "start_time": "2020-10-08T01:47:31.173212Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diabetes = pd.read_csv('data/diabetes_limpo.csv', sep = ',')\n",
    "# diabetes.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "diabetes = explanation.to_float(diabetes)\n",
    "\n",
    "model_file_name = 'serialized_model/DT_diabetes.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = diabetes.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in diabetes_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:33.299041Z",
     "start_time": "2020-10-08T01:47:32.672087Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KNeighborsClassifier' object has no attribute 'n_samples_fit_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-dc9915b67b74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m                            \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                            \u001b[0minstance_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                            output_file)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/phd/phd_first_experiments/explanation_helper/expla_helper.py\u001b[0m in \u001b[0;36mexplain_it\u001b[0;34m(model, dataset, target_names, num_features, instance_index, output_file_path)\u001b[0m\n\u001b[1;32m     79\u001b[0m                                      \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                                      \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                                      top_labels = 2)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     exp.save_to_file(output_file_path,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    272\u001b[0m         ).ravel()\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0myss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# for classification, the model needs to provide a list of tuples - classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \"\"\"\n\u001b[1;32m    503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict_proba'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_estimator_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0mn_neighbors\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mn_samples_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples_fit_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_neighbors\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_samples_fit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KNeighborsClassifier' object has no attribute 'n_samples_fit_'"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "heart = pd.read_csv('data/datasets_33180_43520_heart.csv', sep = ',')\n",
    "heart = explanation.to_float(heart)\n",
    "\n",
    "model_file_name = 'serialized_model/KNN_heart.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = heart.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in heart_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:33.300268Z",
     "start_time": "2020-10-08T01:47:08.963Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = pd.read_csv('data/wine_limpo.csv', sep = ',')\n",
    "# wine.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "wine = explanation.to_float(wine)\n",
    "\n",
    "model_file_name = 'serialized_model/KNN_wine.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = wine.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in wine_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:33.301767Z",
     "start_time": "2020-10-08T01:47:08.969Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diabetes = pd.read_csv('data/diabetes_limpo.csv', sep = ',')\n",
    "# diabetes.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "diabetes = explanation.to_float(diabetes)\n",
    "\n",
    "model_file_name = 'serialized_model/KNN_diabetes.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = diabetes.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in diabetes_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:33.303800Z",
     "start_time": "2020-10-08T01:47:08.979Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "heart = pd.read_csv('data/datasets_33180_43520_heart.csv', sep = ',')\n",
    "heart = explanation.to_float(heart)\n",
    "\n",
    "model_file_name = 'serialized_model/GaussianProcessClassifier_heart.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = heart.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in heart_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:33.307073Z",
     "start_time": "2020-10-08T01:47:08.985Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = pd.read_csv('data/wine_limpo.csv', sep = ',')\n",
    "# wine.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "wine = explanation.to_float(wine)\n",
    "\n",
    "model_file_name = 'serialized_model/GaussianProcessClassifier_wine.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = wine.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in wine_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:33.308858Z",
     "start_time": "2020-10-08T01:47:08.992Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diabetes = pd.read_csv('data/diabetes_limpo.csv', sep = ',')\n",
    "# diabetes.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "diabetes = explanation.to_float(diabetes)\n",
    "\n",
    "model_file_name = 'serialized_model/GaussianProcessClassifier_diabetes.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = diabetes.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in diabetes_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:49:24.228411Z",
     "start_time": "2020-10-08T01:49:23.878905Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SVC' object has no attribute '_probA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-25903d927e91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m                            \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                            \u001b[0minstance_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                            output_file)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/phd/phd_first_experiments/explanation_helper/expla_helper.py\u001b[0m in \u001b[0;36mexplain_it\u001b[0;34m(model, dataset, target_names, num_features, instance_index, output_file_path)\u001b[0m\n\u001b[1;32m     79\u001b[0m                                      \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                                      \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                                      top_labels = 2)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     exp.save_to_file(output_file_path,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    272\u001b[0m         ).ravel()\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0myss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# for classification, the model needs to provide a list of tuples - classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \"\"\"\n\u001b[1;32m    503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict_proba'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_estimator_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobA_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobB_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             raise NotFittedError(\"predict_proba is not available when fitted \"\n\u001b[1;32m    664\u001b[0m                                  \"with probability=False\")\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mprobA_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprobA_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_probA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SVC' object has no attribute '_probA'"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "heart = pd.read_csv('data/datasets_33180_43520_heart.csv', sep = ',')\n",
    "heart = explanation.to_float(heart)\n",
    "\n",
    "model_file_name = 'serialized_model/svm_heart.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = heart.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in heart_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:49:42.472529Z",
     "start_time": "2020-10-08T01:49:42.154652Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SVC' object has no attribute '_probA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9f106d900a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m                            \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                            \u001b[0minstance_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                            output_file)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/phd/phd_first_experiments/explanation_helper/expla_helper.py\u001b[0m in \u001b[0;36mexplain_it\u001b[0;34m(model, dataset, target_names, num_features, instance_index, output_file_path)\u001b[0m\n\u001b[1;32m     79\u001b[0m                                      \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                                      \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                                      top_labels = 2)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     exp.save_to_file(output_file_path,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    272\u001b[0m         ).ravel()\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0myss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# for classification, the model needs to provide a list of tuples - classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \"\"\"\n\u001b[1;32m    503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict_proba'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_estimator_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobA_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobB_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             raise NotFittedError(\"predict_proba is not available when fitted \"\n\u001b[1;32m    664\u001b[0m                                  \"with probability=False\")\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mprobA_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprobA_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_probA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SVC' object has no attribute '_probA'"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = pd.read_csv('data/wine_limpo.csv', sep = ',')\n",
    "# wine.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "wine = explanation.to_float(wine)\n",
    "\n",
    "model_file_name = 'serialized_model/svm_wine.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = wine.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in wine_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:49:46.948435Z",
     "start_time": "2020-10-08T01:49:46.702844Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SVC' object has no attribute '_probA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4fa90f0d64dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m                            \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                            \u001b[0minstance_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                            output_file)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/phd/phd_first_experiments/explanation_helper/expla_helper.py\u001b[0m in \u001b[0;36mexplain_it\u001b[0;34m(model, dataset, target_names, num_features, instance_index, output_file_path)\u001b[0m\n\u001b[1;32m     79\u001b[0m                                      \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                                      \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                                      top_labels = 2)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     exp.save_to_file(output_file_path,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    272\u001b[0m         ).ravel()\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0myss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# for classification, the model needs to provide a list of tuples - classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \"\"\"\n\u001b[1;32m    503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict_proba'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_estimator_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobA_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobB_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             raise NotFittedError(\"predict_proba is not available when fitted \"\n\u001b[1;32m    664\u001b[0m                                  \"with probability=False\")\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mprobA_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprobA_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_probA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SVC' object has no attribute '_probA'"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import sys\n",
    "sys.path.append('./ml_helper')\n",
    "import ml_helper_train_model as model_train\n",
    "import ml_helper_serialize_model as serializer\n",
    "sys.path.append('./explanation_helper')\n",
    "import expla_helper as explanation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diabetes = pd.read_csv('data/diabetes_limpo.csv', sep = ',')\n",
    "# diabetes.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "diabetes = explanation.to_float(diabetes)\n",
    "\n",
    "model_file_name = 'serialized_model/svm_diabetes.pkl'\n",
    "model = serializer.load_model(model_file_name)\n",
    "\n",
    "dataset = diabetes.copy(deep = True)\n",
    "target_names = np.array(['doente', 'saudável'])\n",
    "\n",
    "folder = 'explanation/'\n",
    "model_dataset = model_file_name.replace('serialized_model/','').replace('.pkl', '')\n",
    "\n",
    "for sample in diabetes_sample:\n",
    "    instance_index = sample\n",
    "    num_features = dataset.columns.size - 1 #removing the target\n",
    "\n",
    "    output_file = \"\"\"{0}{1}_instance_{2}_num_features_{3}.html\"\"\".format(folder, \n",
    "                                                                          model_dataset, \n",
    "                                                                          instance_index, \n",
    "                                                                          num_features)\n",
    "\n",
    "    explanation.explain_it(model,\n",
    "                           dataset,\n",
    "                           np.array(['doente', 'saudável']),\n",
    "                           num_features,\n",
    "                           instance_index,\n",
    "                           output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:33.315946Z",
     "start_time": "2020-10-08T01:47:09.030Z"
    }
   },
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "# import shap\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # print the JS visualization code to the notebook\n",
    "# shap.initjs()\n",
    "\n",
    "# # train a SVM classifier\n",
    "# X_train,X_test,Y_train,Y_test = train_test_split(*shap.datasets.iris(), test_size=0.2, random_state=0)\n",
    "# svm = sklearn.svm.SVC(kernel='rbf', probability=True)\n",
    "# svm.fit(X_train, Y_train)\n",
    "\n",
    "# # use Kernel SHAP to explain test set predictions\n",
    "# explainer = shap.KernelExplainer(svm.predict_proba, X_train, link=\"logit\")\n",
    "# shap_values = explainer.shap_values(X_test, nsamples=100)\n",
    "\n",
    "# # plot the SHAP values for the Setosa output of the first instance\n",
    "# shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link=\"logit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:33.318079Z",
     "start_time": "2020-10-08T01:47:09.035Z"
    }
   },
   "outputs": [],
   "source": [
    "# shap.force_plot(explainer.expected_value[0], shap_values[0], X_test, link=\"logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T01:47:33.328810Z",
     "start_time": "2020-10-08T01:47:09.041Z"
    }
   },
   "outputs": [],
   "source": [
    "# shap.summary_plot(shap_values, X_test.iloc[0:1000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meu exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
